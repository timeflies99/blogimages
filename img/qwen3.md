
# Qwen3 浅析：划时代的多语言、多模式、开源大模型


> **开源协议**：Apache 2.0\
> **模型规模**：0.6B ~ 235B 参数 | Dense + MoE 双架构\
> **支持语言**：119种（全球覆盖）\
> **核心能力**：动态思考模式切换 + 强到弱蒸馏 + 高效MoE + 36T Token预训练\
> **报告地址**：[Qwen3\_Technical\_Report.pdf](https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf)

***

## 一、Qwen3 的四大核心创新

### 1️⃣ 动态“思考模式”切换 —— 一模型，两模式

| 模式         | 适用场景                  | 特点                         | 控制方式               |
|--------------|---------------------------|------------------------------|------------------------|
| **思考模式** | 数学证明、代码调试、复杂推理 | 多步链式推理，深度思考       | `/think` 或默认开启    |
| **非思考模式** | 日常对话、快速问答         | 低延迟响应，轻量推理         | `/no think` 指令控制   |
| **思维预算** | 精准控制资源消耗           | 设定最大 token 数，自动截断  | `thinking_budget=512`  |

> **革命性价值**：无需部署多个模型，动态适配任务复杂度，兼顾性能与成本。

***

### 2️⃣ 119种语言支持 —— 真正的全球化模型

* 语言数量从 Qwen2.5 的 **29 → 119种**，覆盖主流语种 + 方言。
* 预训练数据：**36万亿 token**，含代码、STEM、多语言文本、合成数据。
* 数据构建创新：
  * 使用 **Qwen2.5-VL** 从 PDF 提取文本。
  * 使用 **Qwen2.5/Math/Coder** 合成教科书、QA、代码片段。
  * 建立**多维度标注系统**（教育性、领域、安全性），实现**实例级数据混合优化**。

***

### 3️⃣ 高效训练与推理 —— 小模型媲美大模型

* **强到弱蒸馏（Strong-to-Weak Distillation）**：
  * 用大模型（如235B）蒸馏小模型（如4B）。
  * **训练成本降低90%**（仅需1/10 GPU小时）。
  * 小模型性能反超上一代更大模型（如 Qwen3-4B > Qwen2.5-7B）。

* **MoE 架构革新**：
  * 总参235B，激活仅22B（Qwen3-235B-A22B）。
  * **128专家，每token激活8个**。
  * **无共享专家** → 强制专家专业化 → 提升任务性能。
  * 使用 **全局负载均衡损失** 保证专家利用率。

***

### 4️⃣ 全系列开源 + 可复现

* 所有模型（0.6B~235B）**全部开源**，Apache 2.0 协议。
* 提供完整训练策略、超参缩放律、数据构建方法 → **工业级可复现性**。

***

## 二、模型架构：密集与MoE协同进化



| **模型类型** | **型号** | **架构/关键技术** | **关键改进 / 创新点** |
|--------------|----------|--------------------|------------------------|
| **Dense（密集模型）**<br>（共6个） | Qwen3-0.6B<br>Qwen3-1.7B<br>Qwen3-4B<br>Qwen3-8B<br>Qwen3-14B<br>Qwen3-32B | 继承自 Qwen2.5：<br>• Grouped Query Attention (GQA)<br>• SwiGLU 激活函数<br>• Rotary Positional Embeddings (RoPE)<br>• RMSNorm + Pre-normalization | • **移除 QKV-bias** ❌<br>• **新增 QK-Norm** ✅ → 提升训练稳定性 |
| **MoE（混合专家模型）**<br>（共2个） | Qwen3-30B-A3B<br>Qwen3-235B-A22B | 基于 Qwen2.5-MoE：<br>• 细粒度专家分割 | • **无共享专家** ❌ → 所有专家必须专业化<br>• **全局负载均衡损失** ✅ → 避免专家“偷懒”，提升利用率<br>• 每 token 激活 **8 个专家**（来自 **128 专家池**）<br>• **效果**：以更少激活参数，实现更高任务性能 |


> 效果：以更少激活参数，实现更高任务性能。



### Tokenizer：字节级BPE，15万词表

* 使用 Qwen 原生 tokenizer
* **Byte-level BPE** → 更好处理多语言、罕见词、代码
* 词表大小：**151,669** → 平衡效率与覆盖率

***

## 三、预训练：36T Token + 三阶段策略

### 3.1 数据规模与构建

| 维度       | Qwen2.5      | Qwen3         | 提升幅度 |
|------------|--------------|---------------|----------|
| Token量    | ~18T         | **36T**       | 2x       |
| 语言数     | 29           | **119**       | 4x       |
| 数据来源   | 通用文本     | +PDF提取、合成数据、多模态标注 | 更高质量、更多样 |

* **合成数据引擎**：
  * 利用 Qwen2.5 系列模型自动生成高质量 QA、代码、教科书。
* **数据标注系统**：
  * 对 >30T token 进行多维度标注（教育性、领域、安全性）。
  * **实例级数据混合优化** → 比传统“领域级”混合更精细、更高效。



### 3.2 三阶段预训练策略

| 阶段             | 目标                     | 序列长度   | 数据量     | 关键技术                     |
|------------------|--------------------------|------------|------------|------------------------------|
| **S1 通用阶段**  | 语言能力 + 世界知识      | 4,096      | >30T token | 覆盖119语言                  |
| **S2 推理阶段**  | 强化STEM/代码/推理       | 4,096      | ~5T token  | 增加合成数据比例，加速LR衰减 |
| **S3 长上下文**  | 支持32K上下文            | 32,768     | 高质量长文 | ABF + YARN + Dual Chunk Attention |


**长上下文关键技术**：
* **ABF**：RoPE 基频从 10K → 1M
* **YARN**：动态扩展上下文窗口
* **Dual Chunk Attention**：推理时序列容量提升4倍



### 3.3 预训练评估

在15个基准测试中评估：

* 通用知识
* 数学与STEM
* 代码生成
* 多语言理解

为后续“缩放律预测”提供数据基础 → 自动推荐最优学习率、batch size。

***

## 四、后训练：四阶段精炼 + 强到弱蒸馏

### 核心目标：

1. 实现**思考模式控制**
2. 通过**蒸馏降低小模型训练成本**



### 4.1 阶段一：长CoT冷启动（Long-CoT Cold Start）

* **目标**：注入基础推理能力，不追求即时性能
* **数据构建**：
  * 覆盖数学、代码、逻辑、STEM
  * 严格两阶段过滤：
    * 查询过滤：移除易答/不可验证问题
    * 响应过滤：确保CoT质量
* **方法**：监督微调（SFT）



### 4.2 阶段二：推理强化学习（Reasoning RL）

* **数据要求**：
  * 未在阶段一使用
  * 对当前模型“可学习”
  * 具挑战性 + 覆盖子领域
* **数量**：3,995 高质量 query-verify 对
* **算法**：GRPO
* **技巧**：
  * 大batch + 多rollout
  * 离线训练提升样本效率
  * 控制熵平衡探索/利用
* **效果**：Qwen3-235B-A22B 在 AIME’24 从 70.1 → **85.1**



### 4.3 阶段三：思维模式融合（Thinking Mode Fusion）

**核心创新阶段**：让一个模型同时掌握“思考”与“非思考”能力

#### SFT 数据构建

* **思考数据**：用阶段二模型自身生成（拒绝采样保证质量）
* **非思考数据**：覆盖指令、多语言、创意、角色扮演等
* 特别增强：低资源语言翻译任务

#### 聊天模板设计

```
用户：/think 请证明勾股定理。
助手：[思考中...] ... [结论] ...

用户：/no think 今天天气如何？
助手：晴，25度，适合外出。
```

* 默认模式：**思考模式**
* 多轮对话：**遵循最后一个标志**
* 即使非思考模式，也保留空思考块 → 保持内部格式一致

#### 思维预算（Thinking Budget）

* 非显式训练，而是**自然涌现能力**
* 当思考token超限 → 自动插入停止指令 → 基于已有推理生成答案
* 实现**资源可控的渐进式推理**



### 4.4 阶段四：通用强化学习（General RL）

* **目标**：提升通用能力、稳定性、对齐性
* **奖励系统**：覆盖20+任务，定制评分标准
  * 指令遵循
  * 格式遵循
  * 偏好对齐
  * Agent能力
  * RAG任务
* **三类奖励**：
  1. **规则奖励**（推理、格式）
  2. **带参考模型奖励**（灵活任务）
  3. **无参考偏好奖励**（人类偏好数据训练）



### 4.5 强到弱蒸馏（专为小模型设计）

**革命性效率提升**：小模型训练成本仅为传统方法的 **1/10**

#### 离线蒸馏（Off-policy）

* 教师模型（235B/32B）在 `/think` 和 `/no think` 模式下生成响应
* 学生模型学习响应 → 获得基础推理 + 模式切换能力

#### 在线蒸馏（On-policy）

* 学生模型生成响应（带模式标志）
* 对齐教师模型 logit → 最小化 KL 散度
* 实现性能 + 控制能力双提升

效果：Qwen3-4B > Qwen2.5-7B，且具备完整思考模式切换能力

***

## 五、性能评估：全面超越，开源之王

### 旗舰模型 Qwen3-235B-A22B 表现

| 领域       | 基准               | 得分    | 对比模型         | 结果       |
|------------|--------------------|---------|------------------|------------|
| **数学**   | AIME’24            | **85.7** | DeepSeek-R1 (79.8) | ✅ 超越     |
| **代码**   | LiveCodeBench v5   | **70.7** | Gemini 2.5 Pro   | ✅ 比肩     |
| **多语言** | MT-AIME2024 (55语) | **80.8** | —                | 🌍 领先     |



### 轻量模型表现（蒸馏策略验证）

| 模型          | 能力表现                     | 对比对象       | 结论               |
|---------------|------------------------------|----------------|--------------------|
| Qwen3-4B      | 多项任务超越                 | Qwen2.5-7B     | ✅ 蒸馏有效，小胜大 |
| Qwen3-30B-A3B | 高效MoE，性能接近32B密集模型 | Qwen3-32B      | ✅ 激活参数效率高   |



### 性能权衡说明

* **思维模式融合 + 通用RL** 后：
  * 通用能力、指令遵循、Agent能力 ↑
  * 部分高难度数学/代码任务性能轻微下降

* **官方解释**：
  为增强模型**整体多功能性**，接受在专业任务上的轻微性能权衡。

***

## 六、总结与展望

### Qwen3 的划时代意义：

1. **首个支持动态思考模式切换的开源大模型** → 一模型通吃简单与复杂任务。
2. **119语言支持 + 36T Token训练** → 真正全球化、多领域能力。
3. **强到弱蒸馏 + 高效MoE** → 小模型低成本高性能，推动边缘部署。
4. **全系列开源 + 完整技术报告** → 社区可复现、可改进、可商用。



### 未来展望：

* **多模态扩展**：结合 Qwen-VL，打造统一多模态智能体。
* **工具调用增强**：深度集成 LangChain / LangGraph，构建自主Agent。
* **硬件适配优化**：针对国产芯片（昇腾、寒武纪）做推理优化。
* **社区生态建设**：鼓励开发者贡献插件、微调方案、垂直领域模型。
